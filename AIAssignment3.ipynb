{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Komal-Zia/PAIassignment03/blob/main/AIAssignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axf68ED8cV5j",
        "outputId": "7b8b77cc-5b4a-437e-f636-c1d571d1cf0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7FQQOldcs8t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kC3ggtBc3Aa"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train_binary_sent.csv')\n",
        "dev_df = pd.read_csv('/content/dev_binary_sent.csv')\n",
        "test_df = pd.read_csv('/content/test_binary_sent.csv')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "max_len = 128\n",
        "train_dataset = SentimentDataset(train_df['sentence'], train_df['label'], tokenizer, max_len)\n",
        "dev_dataset = SentimentDataset(dev_df['sentence'], dev_df['label'], tokenizer, max_len)\n",
        "test_dataset = SentimentDataset(test_df['sentence'], test_df['label'], tokenizer, max_len)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdiTzYiRc9S9"
      },
      "outputs": [],
      "source": [
        "#Define RNN_BERT Model\n",
        "class RNNBERTModel(nn.Module):\n",
        "    def __init__(self, hidden_size, num_classes):\n",
        "        super(RNNBERTModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.bert.config.hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Bidirectional\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():  # Freeze BERT weights\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        rnn_output, _ = self.rnn(bert_output.last_hidden_state)\n",
        "        output = self.fc(rnn_output[:, 0, :])  # Use the output of the first token ([CLS])\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV9str2ddBQ2"
      },
      "outputs": [],
      "source": [
        "#Training Function\n",
        "def train_model(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkUugbF3dGSU"
      },
      "outputs": [],
      "source": [
        "#Evaluation Function\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "    return accuracy, precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8oeXz6edXHc",
        "outputId": "2801c7f0-a352-4c42-894e-107a7c7f0537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 0.5980, Dev Accuracy = 0.8268, Precision = 0.8097, Recall = 0.8626, F1 Score = 0.8353\n",
            "Epoch 2: Train Loss = 0.3799, Dev Accuracy = 0.8498, Precision = 0.8366, Recall = 0.8761, F1 Score = 0.8559\n",
            "Epoch 3: Train Loss = 0.3449, Dev Accuracy = 0.8475, Precision = 0.8104, Recall = 0.9144, F1 Score = 0.8593\n"
          ]
        }
      ],
      "source": [
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RNNBERTModel(hidden_size=256, num_classes=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the model\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
        "    dev_accuracy, dev_precision, dev_recall, dev_f1 = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Dev Accuracy = {dev_accuracy:.4f}, \"\n",
        "          f\"Precision = {dev_precision:.4f}, Recall = {dev_recall:.4f}, F1 Score = {dev_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(model, test_loader, device)\n",
        "print(f\"Test Results: Accuracy = {test_accuracy:.4f}, Precision = {test_precision:.4f}, \"\n",
        "      f\"Recall = {test_recall:.4f}, F1 Score = {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvEvOZSxofrX",
        "outputId": "4372b066-352c-41d4-ad42-92b4a530fa2b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results: Accuracy = 0.8402, Precision = 0.8059, Recall = 0.8955, F1 Score = 0.8484\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFD/FQ0KWHGB5zS1bTBHBj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}